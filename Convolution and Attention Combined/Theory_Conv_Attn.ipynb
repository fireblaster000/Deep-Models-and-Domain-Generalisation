{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Depthwise Convolution\n",
        "---\n",
        "**Equation:**\n",
        "\n",
        "$$\n",
        "y_i = \\sum_{j \\in \\mathcal{L}(i)} W_{i-j} \\odot X_j\n",
        "$$\n",
        "\n",
        "- **Explanation:**\n",
        "    - In depthwise convolution, we apply a kernel $W$ to a local neighborhood $\\mathcal{L}(i)$ of pixel $i$. The kernel weights $W_{i-j}$ are multiplied with the corresponding pixel values $X_j$ in this neighborhood.\n",
        "    - This means each pixel $i$ gathers information only from its nearby pixels defined by the size of the kernel (e.g., 3x3, 5x5).\n",
        "\n",
        "- **Receptive field:**\n",
        "    - The receptive field is **local**, determined by the kernel size. This limits the information gathering to small neighborhoods of pixels.\n",
        "    - For a K × K kernel, each output pixel in the feature map has a receptive field of K × K in the input image.\n",
        "    - Increasing the filter size K directly increases the receptive field, allowing the model to capture larger local patterns. Conversely, reducing K decreases the receptive field, focusing on smaller details.\n",
        "\n",
        "- **Information Propogation:**\n",
        "  - Information from neighboring pixels is combined based on the weights defined by the filter, allowing the model to learn local features effectively. Each output pixel is influenced solely by the pixels within its receptive field.\n",
        "---\n",
        "**Example in Python:**\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Sample image and kernel for depthwise convolution\n",
        "image = np.random.rand(5, 5)\n",
        "kernel = np.ones((3, 3))  # 3x3 kernel\n",
        "\n",
        "# Apply depthwise convolution using OpenCV\n",
        "output = cv2.filter2D(image, -1, kernel)\n",
        "\n",
        "output\n"
      ],
      "metadata": {
        "id": "KlJdxukxe_F1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Self-Attention\n",
        "\n",
        "---\n",
        "**Equation:**\n",
        "\n",
        "$$\n",
        "y_i = \\sum_{j \\in \\mathcal{G}} \\frac{\\exp(X_i^T X_j)}{\\sum_{k \\in \\mathcal{G}} \\exp(X_i^T X_k)} x_j\n",
        "$$\n",
        "\n",
        "- **Explanation:**\n",
        "    - In self-attention, every pixel $i$ attends to all the other pixels $j \\in \\mathcal{G}$ in the image. The contribution of each pixel $j$ to the final value at pixel $i$ is weighted by the similarity score $X_i^T X_j$, normalized across all pixels.\n",
        "    - The similarity score captures how much the value of pixel $j$ should contribute to the output at pixel $i$, based on the inner product of their feature representations.\n",
        "    \n",
        "- **Receptive field:**\n",
        "    - The receptive field in self-attention is **global**, meaning pixel $i$ integrates information from **all other pixels** in the image. This allows the model to capture long-range dependencies and interactions, unlike convolution, which is limited to a local neighborhood.\n",
        "    - The size of the receptive field does not depend on kernel size, as all pixels can interact with each other directly. The receptive field is inherently the entire input image, allowing for complex relationships to be captured regardless of distance.\n",
        "\n",
        "- **Information Propogation:**\n",
        "    - The attention mechanism is content-aware: information is aggregated based on the relevance of the surrounding pixels, not just their spatial proximity. Information flows globally, allowing the model to capture relationships between distant pixels. This capability enhances the contextual understanding of image components.\n",
        "    - The global nature of the receptive field makes self-attention suitable for tasks where context from the entire image is important.\n",
        "\n",
        "---\n",
        "\n",
        "**Example in Python:**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Define a simple self-attention mechanism\n",
        "def self_attention(X):\n",
        "    \"\"\"\n",
        "    A simplified self-attention function.\n",
        "    X: Input feature matrix (each row is a pixel's feature vector)\n",
        "    Returns: Weighted sum of input features using self-attention\n",
        "    \"\"\"\n",
        "    # Compute the similarity scores (dot-product attention)\n",
        "    scores = np.exp(X @ X.T)  # Shape: (N, N), where N is the number of pixels\n",
        "\n",
        "    # Normalize the attention scores by rows\n",
        "    attention_weights = scores / np.sum(scores, axis=1, keepdims=True)\n",
        "\n",
        "    # Compute the attention-weighted output\n",
        "    output = attention_weights @ X\n",
        "\n",
        "    return output\n",
        "\n",
        "# Example input (5 pixels with 5-dimensional feature vectors)\n",
        "X = np.random.rand(5, 5)\n",
        "\n",
        "# Apply self-attention\n",
        "attention_output = self_attention(X)\n",
        "\n",
        "# Display the output\n",
        "attention_output\n"
      ],
      "metadata": {
        "id": "WJmGuO_ogbbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Post-normalization Combination\n",
        "---\n",
        "**Equation:**\n",
        "\n",
        "$$\n",
        "y_i^{post} = \\sum_{j \\in \\mathcal{G}} \\left( \\frac{\\exp(X_i^T X_j)}{\\sum_{k \\in \\mathcal{G}} \\exp(X_i^T X_k)} + W_{i-j} \\right) x_j\n",
        "$$\n",
        "\n",
        "- **Explanation:**\n",
        "    - In this method, we combine the results of **self-attention** and **depthwise convolution** after normalizing the attention scores.\n",
        "    - The self-attention term allows pixel $i$ to aggregate global information from all pixels $j$, while the convolution term adds local information from nearby pixels.\n",
        "    - The result is a blend of global (attention) and local (convolution) information at pixel $i$.\n",
        "\n",
        "- **Receptive field:**\n",
        "    - The receptive field here is both **local** (from the convolution) and **global** (from the self-attention), giving the model access to both nearby and distant pixels.\n",
        "\n",
        "- **Information Propogation:**\n",
        "  - Local features are extracted separately through convolution, which are then contextualized by the global information obtained through self-attention. This combination enhances the model's capability to learn complex features.\n",
        "\n",
        "---\n",
        "\n",
        "**Example in Python:**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Define a post-normalization combination function\n",
        "def post_normalization_combination(X, kernel):\n",
        "    \"\"\"\n",
        "    Combines self-attention and depthwise convolution post-normalization.\n",
        "    X: Input feature matrix (each row is a pixel's feature vector)\n",
        "    kernel: Convolution kernel\n",
        "    Returns: Combined feature map\n",
        "    \"\"\"\n",
        "    # Compute the self-attention scores\n",
        "    attention_scores = np.exp(X @ X.T)\n",
        "    \n",
        "    # Normalize the attention scores\n",
        "    attention_weights = attention_scores / np.sum(attention_scores, axis=1, keepdims=True)\n",
        "    \n",
        "    # Apply depthwise convolution\n",
        "    conv_output = cv2.filter2D(X, -1, kernel)\n",
        "    \n",
        "    # Combine self-attention and convolution post-normalization\n",
        "    combined_output = attention_weights @ X + conv_output\n",
        "    \n",
        "    return combined_output\n",
        "\n",
        "# Example input (5 pixels with 5-dimensional feature vectors)\n",
        "X = np.random.rand(5, 5)\n",
        "kernel = np.random.rand(3, 3)\n",
        "\n",
        "# Apply post-normalization combination\n",
        "post_norm_output = post_normalization_combination(X, kernel)\n",
        "\n",
        "# Display the output\n",
        "post_norm_output\n"
      ],
      "metadata": {
        "id": "uvIGrgIfhPdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Pre-normalization Combination\n",
        "---\n",
        "**Equation:**\n",
        "\n",
        "$$\n",
        "y_i^{pre} = \\sum_{j \\in \\mathcal{G}} \\frac{\\exp(X_i^T X_j + W_{i-j})}{\\sum_{k \\in \\mathcal{G}} \\exp(X_i^T X_k + W_{i-k})} x_j\n",
        "$$\n",
        "\n",
        "- **Explanation:**\n",
        "    - In this method, the convolution weights $W_{i-j}$ and self-attention scores $X_i^T X_j$ are combined **before normalization**.\n",
        "    - The convolution kernel directly influences the attention mechanism by adjusting the attention scores prior to softmax normalization.\n",
        "    - The self-attention term still enables global information propagation, while the convolution kernel ensures that local information is preserved.\n",
        "  \n",
        "- **Receptive field:**\n",
        "    - The receptive field is a mix of **local** (convolution) and **global** (self-attention) interactions.\n",
        "    - The pre-normalization combination ensures that convolutional features contribute to the attention score calculation, giving more importance to specific local features before normalizing them across the global field.\n",
        "\n",
        "- **Information Propogation:**\n",
        "  - The normalization stabilizes training and enhances the quality of features extracted by both operations, allowing for more effective learning.\n",
        "\n",
        "---\n",
        "\n",
        "**Example in Python:**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Define a pre-normalization combination function\n",
        "def pre_normalization_combination(X, kernel):\n",
        "    \"\"\"\n",
        "    Combines self-attention and depthwise convolution pre-normalization.\n",
        "    X: Input feature matrix (each row is a pixel's feature vector)\n",
        "    kernel: Convolution kernel\n",
        "    Returns: Combined feature map\n",
        "    \"\"\"\n",
        "    # Compute the combined self-attention + convolution scores pre-normalization\n",
        "    combined_scores = np.exp(X @ X.T + kernel)\n",
        "    \n",
        "    # Normalize the combined scores\n",
        "    attention_weights = combined_scores / np.sum(combined_scores, axis=1, keepdims=True)\n",
        "    \n",
        "    # Apply the attention weights to the input\n",
        "    combined_output = attention_weights @ X\n",
        "    \n",
        "    return combined_output\n",
        "\n",
        "# Example input (5 pixels with 5-dimensional feature vectors)\n",
        "X = np.random.rand(5, 5)\n",
        "kernel = np.random.rand(5, 5)  # Assuming kernel is the same shape as X for simplicity\n",
        "\n",
        "# Apply pre-normalization combination\n",
        "pre_norm_output = pre_normalization_combination(X, kernel)\n",
        "\n",
        "# Display the output\n",
        "pre_norm_output\n"
      ],
      "metadata": {
        "id": "V_q4Jwm4jaP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Attention Modulated Convolution\n",
        "---\n",
        "**Equation:**\n",
        "\n",
        "$$\n",
        "Y(i) = \\sum_{j \\in \\mathcal{L}(i)} A(i, j) \\odot X(j) \\odot W(i - j)\n",
        "$$\n",
        "\n",
        "where the attention weight $A(i, j)$ is given as:\n",
        "\n",
        "$$\n",
        "A(i, j) = \\frac{\\exp(\\text{score}(Q(i), K(j)))}{\\sum_{k \\in \\mathcal{L}(i)} \\exp(\\text{score}(Q(i), K(k)))}\n",
        "$$\n",
        "\n",
        "- **Explanation:**\n",
        "    - In **Attention Modulated Convolution**, the convolution process is modulated by attention weights $A(i, j)$, which dynamically scale the contribution of each neighboring pixel $j$ in the convolution operation.\n",
        "    - The attention weights $A(i, j)$ are computed using a similarity score (e.g., dot-product) between the query $Q(i)$ at pixel $i$ and the key $K(j)$ at neighboring pixel $j$.\n",
        "    - The attention mechanism adjusts the contribution of each neighboring pixel based on how relevant it is to the current pixel $i$, making the convolution adaptive to the content of the image.\n",
        "\n",
        "- **Receptive field:**\n",
        "    - The receptive field is **local** (as determined by the convolution filter size), but the attention weights allow for more flexible and content-dependent information aggregation from neighboring pixels.\n",
        "\n",
        "- **Information Propogation:**\n",
        "  - This operation emphasizes important local features by modulating them with global attention, effectively balancing local detail and global context.\n",
        "\n",
        "---\n",
        "\n",
        "**Example in Python:**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Define the attention modulated convolution function\n",
        "def attention_modulated_convolution(X, kernel, Q, K):\n",
        "    \"\"\"\n",
        "    Applies attention modulated convolution.\n",
        "    X: Input feature matrix (each row is a pixel's feature vector)\n",
        "    kernel: Convolution kernel\n",
        "    Q: Query matrix (for attention)\n",
        "    K: Key matrix (for attention)\n",
        "    Returns: Convolution result modulated by attention.\n",
        "    \"\"\"\n",
        "    # Compute attention scores (dot product of query and key)\n",
        "    attention_scores = np.exp(Q @ K.T)\n",
        "    \n",
        "    # Normalize the attention scores\n",
        "    attention_weights = attention_scores / np.sum(attention_scores, axis=1, keepdims=True)\n",
        "    \n",
        "    # Apply depthwise convolution using OpenCV\n",
        "    conv_output = cv2.filter2D(X, -1, kernel)\n",
        "    \n",
        "    # Modulate convolution output by attention weights\n",
        "    attention_modulated_output = attention_weights @ conv_output\n",
        "    \n",
        "    return attention_modulated_output\n",
        "\n",
        "# Example input (5 pixels with 5-dimensional feature vectors)\n",
        "X = np.random.rand(5, 5)\n",
        "kernel = np.random.rand(3, 3)  # A random convolution kernel\n",
        "\n",
        "# Example query and key matrices (for attention)\n",
        "Q = np.random.rand(5, 5)\n",
        "K = np.random.rand(5, 5)\n",
        "\n",
        "# Apply attention modulated convolution\n",
        "attention_conv_output = attention_modulated_convolution(X, kernel, Q, K)\n",
        "\n",
        "# Display the output\n",
        "attention_conv_output\n"
      ],
      "metadata": {
        "id": "0wnbfhDIleIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Convolution Modulated Attention\n",
        "---\n",
        "**Equation:**\n",
        "\n",
        "$$\n",
        "\\text{Attention}(X) = \\text{Softmax}\\left( \\text{DWConv}_{F \\times F}(X, W) \\right) \\odot V\n",
        "$$\n",
        "\n",
        "- **Explanation:**\n",
        "    - In **Convolution Modulated Attention**, depthwise convolution (denoted as $\\text{DWConv}_{F \\times F}$) is first applied to the input feature map $X$ using a convolution kernel $W$. This captures all the local spatial relationships.\n",
        "    - The result of the depthwise convolution is passed through a softmax function, which transforms it into attention weights.\n",
        "    - These attention weights are then applied to another input feature map $V$ to generate the final modulated output.\n",
        "    - The convolution operation allows local features to be aggregated, while the attention mechanism ensures that the features are modulated based on their relevance.\n",
        "\n",
        "- **Receptive field:**\n",
        "    - The receptive field is **local**, determined by the convolution kernel size, but the attention mechanism allows for a more adaptive combination of local features.\n",
        "\n",
        "- **Information Propogation:**\n",
        "  - Local features are transformed and then contextualized by self-attention, leading to robust feature representation. The combination allows for the model to prioritize relevant features based on the global context.\n",
        "  - The process starts by capturing information locally using depthwise convolution, and then these local features are modulated by attention. This allows the model to adaptively weight the local features, enhancing its ability to focus on the most important parts of the input.\n",
        "\n",
        "---\n",
        "\n",
        "**Example in Python:**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Define the convolution modulated attention function\n",
        "def convolution_modulated_attention(X, kernel, V):\n",
        "    \"\"\"\n",
        "    Applies convolution modulated attention.\n",
        "    X: Input feature matrix (each row is a pixel's feature vector)\n",
        "    kernel: Convolution kernel\n",
        "    V: Input feature map to be modulated by attention\n",
        "    Returns: Attention-modulated feature map.\n",
        "    \"\"\"\n",
        "    # Apply depthwise convolution to input X\n",
        "    conv_output = cv2.filter2D(X, -1, kernel)\n",
        "    \n",
        "    # Compute the attention weights by applying softmax to convolution output\n",
        "    attention_weights = np.exp(conv_output) / np.sum(np.exp(conv_output), axis=1, keepdims=True)\n",
        "    \n",
        "    # Modulate the feature map V with attention weights\n",
        "    attention_modulated_output = attention_weights @ V\n",
        "    \n",
        "    return attention_modulated_output\n",
        "\n",
        "# Example input (5 pixels with 5-dimensional feature vectors)\n",
        "X = np.random.rand(5, 5)\n",
        "kernel = np.random.rand(3, 3)  # A random convolution kernel\n",
        "V = np.random.rand(5, 5)  # A separate feature map to be modulated\n",
        "\n",
        "# Apply convolution modulated attention\n",
        "conv_mod_attn_output = convolution_modulated_attention(X, kernel, V)\n",
        "\n",
        "# Display the output\n",
        "conv_mod_attn_output\n"
      ],
      "metadata": {
        "id": "pd-39ctNmj8J"
      }
    }
  ]
}